## Задание 1
Запуск:
`gunicorn -c wsgi_conf.py  askme_voevodin.wsgi`
## Задание 2
Запуск: `gunicorn -b localhost:8081 test_wsgi`  
Запрос с GET параметрами: `curl -X POST "http://localhost:8081?param1=value1&param2=value2"`  

Запрос с POST параметрами: `curl -X POST "http://localhost:8081" -d "param3=value3&param4=value4"`  

Запрос с GET и POST параметрами: `curl -X POST "http://localhost:8081?param1=value1&param2=value2" -d "param3=value3&param4=value4"`
## Задание 5
### 1. Отдача статического документа напрямую через nginx.
Отключаем все кеширование и выполняем запрос: `ab -n 10000 -c 1000 http://127.0.0.1/static/sample.html`  
**Вывод:**  
Requests per second:    **31920.73** [#/sec] (mean)  
50%     31  
66%     32  
75%     32  
80%     32  
90%     33  
95%     34  
98%     34  
99%     34  
100%    35 (longest request)  
  


### 2. Отдача статического документа напрямую через gunicorn.
Настраиваем django на отдачу статики и выполяем запрос: `ab -n 10000 -c 1000 http://127.0.0.1:8000/static/sample.html`  
**Вывод:**
Requests per second:    **5921.27** [#/sec] (mean)  
50%    165  
66%    166  
75%    166  
80%    167  
90%    167  
95%    167  
98%    168  
99%    169  
100%   173 (longest request)  


### 3. Отдача динамического документа напрямую через gunicorn.
Выполняем запрос: `ab -n 100 -c 10 http://127.0.0.1:8000/`
**Вывод:**
Requests per second:    **35.69** [#/sec] (mean)  
50%    272  
66%    282  
75%    286  
80%    287  
90%    298  
95%    306  
98%    307  
99%    309  
100%   309 (longest request)  


### 4. Отдача динамического документа через проксирование запроса с nginx на gunicorn.
Выполняем запрос: `ab -n 100 -c 10 http://127.0.0.1/`  
**Вывод:**  
Requests per second:    **35.64** [#/sec] (mean)  
50%    276  
66%    279  
75%    283  
80%    284  
90%    288  
95%    292  
98%    297  
99%    303  
100%   303 (longest request)  



### 5. Отдача динамического документа через проксирование запроса с nginx на gunicorn, при кэшировние ответа на nginx (proxy cache).
Возвращаем обратно настройки для кеширования и выполняем запрос: `ab -n 100 -c 10 http://127.0.0.1/`  
**Вывод:**  
Requests per second:    **1686.31** [#/sec] (mean)  
50%      0  
66%      1  
75%      1  
80%      1  
90%      1  
95%      1  
98%      1  
99%     55  
100%    55 (longest request)  
Первый запрос был выполнен честно, остальные взялись из кеша, а так как запросов было мало, то получился маленький RPS.
Чтобы посмотреть, до каких значений он разгонится, выполним следующий запрос: `ab -n 10000 -c 10 http://127.0.0.1/`
**Вывод:**  
Requests per second:    **30074.10** [#/sec] (mean)  
50%      0  
66%      0  
75%      0  
80%      0  
90%      0  
95%      0  
98%      1  
99%      1  
100%    61 (longest request)  
Как видно, теперь RPS намного больше

## Ответы на вопросы
### Насколько быстрее отдается статика по сравнению с WSGI?
**Ответ:** на +- 26к запросов в секунду больше или почти в 5.3 раза

### Во сколько раз ускоряет работу proxy_cache?
**Ответ:** почти в 1000 раз